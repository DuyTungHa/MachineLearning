{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spark.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOItONUQwnyoCSakcwN1IJU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2Z3GmNZHRO7P"},"source":["# Install spark on Google Collab\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://apache.mirror.amaze.com.au/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n","!pip install -q findspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQov5Z-rRUMs"},"source":["# Import Spark environment variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXjaowT4RYE3","executionInfo":{"status":"ok","timestamp":1603275564482,"user_tz":-660,"elapsed":9302,"user":{"displayName":"Duy Tung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXaQhxVwGl7g-mvwhXwJj3o9cSpPflw5NhM3rEYQ=s64","userId":"05049257213918528608"}},"outputId":"d2bd3a44-e0ab-420a-906f-c5aa809b4fc8","colab":{"base_uri":"https://localhost:8080/","height":216}},"source":["# Initialize Spark session\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","spark"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://d8841c3dc279:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.0.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f58f52ff2b0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"itV0QH6yRZ9o","executionInfo":{"status":"ok","timestamp":1603275613199,"user_tz":-660,"elapsed":18081,"user":{"displayName":"Duy Tung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXaQhxVwGl7g-mvwhXwJj3o9cSpPflw5NhM3rEYQ=s64","userId":"05049257213918528608"}},"outputId":"a18fda59-d96a-4236-9e48-c19898293f09","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mount filesystem from Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HybS1tD4Sj2K"},"source":["# Import relevant packages\n","from pyspark.sql.functions import monotonically_increasing_id, desc, col\n","from pyspark.ml.feature import RFormula\n","from pyspark.ml import Pipeline\n","from pyspark.ml.tuning import ParamGridBuilder\n","from pyspark.ml.tuning import TrainValidationSplit\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import StringIndexer, VectorIndexer\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcYAY43QR88Z","executionInfo":{"status":"ok","timestamp":1603275709336,"user_tz":-660,"elapsed":89174,"user":{"displayName":"Duy Tung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXaQhxVwGl7g-mvwhXwJj3o9cSpPflw5NhM3rEYQ=s64","userId":"05049257213918528608"}},"outputId":"5e888feb-ce7d-42a7-9df8-82bace8e3aac","colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["# Load the data into a Spark DataFrame and print the schema\n","df_SUSY = spark.read.format(\"csv\").load(\"/content/gdrive/My Drive/Colab Notebooks/SUSY.csv\", inferSchema=True)\\\n","  .cache()\\\n","  .toDF(\"class\", \"lepton_1_pT\", \"lepton_1_eta\", \"lepton_1_phi\", \"lepton_2_pT\", \"lepton_2_eta\", \"lepton_2_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"MET_rel\", \"axial_MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_DELTA_R\", \"dPhi_r_b\", \"cos_thera_r1\")\n","\n","df_SUSY.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- class: double (nullable = true)\n"," |-- lepton_1_pT: double (nullable = true)\n"," |-- lepton_1_eta: double (nullable = true)\n"," |-- lepton_1_phi: double (nullable = true)\n"," |-- lepton_2_pT: double (nullable = true)\n"," |-- lepton_2_eta: double (nullable = true)\n"," |-- lepton_2_phi: double (nullable = true)\n"," |-- missing_energy_magnitude: double (nullable = true)\n"," |-- missing_energy_phi: double (nullable = true)\n"," |-- MET_rel: double (nullable = true)\n"," |-- axial_MET: double (nullable = true)\n"," |-- M_R: double (nullable = true)\n"," |-- M_TR_2: double (nullable = true)\n"," |-- R: double (nullable = true)\n"," |-- MT2: double (nullable = true)\n"," |-- S_R: double (nullable = true)\n"," |-- M_DELTA_R: double (nullable = true)\n"," |-- dPhi_r_b: double (nullable = true)\n"," |-- cos_thera_r1: double (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8EPZ15KrSe4s"},"source":["# Split the data in to training set and test set\n","# Test set consist of the last 500000 data rows\n","indexed_SUSY = df_SUSY.withColumn(\"index\", monotonically_increasing_id())\n","train_SUSY = df_SUSY.limit(4500000)\n","test_SUSY = indexed_SUSY.orderBy(desc(\"index\")).drop(\"index\").limit(500000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWPoqm1ZSq_L"},"source":["# Create the spark training pipeline\n","rForm = RFormula(formula=\"class ~ .\")\n","lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")\n","\n","stages = [rForm, lr]\n","pipeline = Pipeline().setStages(stages)\n","\n","# Parameter tuning \n","params = ParamGridBuilder()\\\n","    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n","    .addGrid(lr.regParam, [0.1, 2.0])\\\n","    .build()\n","\n","# Use area under ROC as an evaluation metric\n","evaluator = BinaryClassificationEvaluator()\\\n","    .setMetricName(\"areaUnderROC\")\\\n","    .setRawPredictionCol(\"prediction\")\\\n","    .setLabelCol(\"label\")\n","\n","# Split the training dataset into two different groups - training data and validation data\n","tvs = TrainValidationSplit()\\\n","    .setTrainRatio(0.75)\\\n","    .setEstimatorParamMaps(params)\\\n","    .setEstimator(pipeline)\\\n","    .setEvaluator(evaluator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sn7nbk3NSy7B","executionInfo":{"status":"ok","timestamp":1603279057894,"user_tz":-660,"elapsed":410456,"user":{"displayName":"Duy Tung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXaQhxVwGl7g-mvwhXwJj3o9cSpPflw5NhM3rEYQ=s64","userId":"05049257213918528608"}},"outputId":"9873cb3d-fda9-4667-c3e0-123b4b854816","colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["# Train the logistic regression model with parameter tuning\n","tvsFitteds = []\n","# Split the dataframe into multiple smaller datasets for different classifiers\n","dfs = train_SUSY.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2])\n","\n","for df_i in dfs:\n","  tvsFitted = tvs.fit(df_i)\n","  # Print out the resulting model after training \n","  print(type(tvsFitted))\n","  print(tvsFitted.bestModel)\n","  print(tvsFitted.bestModel.stages)\n","  # Add the classifier to the list of classifiers\n","  tvsFitteds.append(tvsFitted)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pyspark.ml.tuning.TrainValidationSplitModel'>\n","PipelineModel_24370f6b1296\n","[RFormulaModel: uid=RFormula_59063e31b2ca, resolvedFormula=ResolvedRFormula(label=class, terms=[lepton_1_pT,lepton_1_eta,lepton_1_phi,lepton_2_pT,lepton_2_eta,lepton_2_phi,missing_energy_magnitude,missing_energy_phi,MET_rel,axial_MET,M_R,M_TR_2,R,MT2,S_R,M_DELTA_R,dPhi_r_b,cos_thera_r1], hasIntercept=true), LogisticRegressionModel: uid=LogisticRegression_99162fba90c1, numClasses=2, numFeatures=18]\n","<class 'pyspark.ml.tuning.TrainValidationSplitModel'>\n","PipelineModel_48f4d0b296d2\n","[RFormulaModel: uid=RFormula_59063e31b2ca, resolvedFormula=ResolvedRFormula(label=class, terms=[lepton_1_pT,lepton_1_eta,lepton_1_phi,lepton_2_pT,lepton_2_eta,lepton_2_phi,missing_energy_magnitude,missing_energy_phi,MET_rel,axial_MET,M_R,M_TR_2,R,MT2,S_R,M_DELTA_R,dPhi_r_b,cos_thera_r1], hasIntercept=true), LogisticRegressionModel: uid=LogisticRegression_99162fba90c1, numClasses=2, numFeatures=18]\n","<class 'pyspark.ml.tuning.TrainValidationSplitModel'>\n","PipelineModel_b8788e75780b\n","[RFormulaModel: uid=RFormula_59063e31b2ca, resolvedFormula=ResolvedRFormula(label=class, terms=[lepton_1_pT,lepton_1_eta,lepton_1_phi,lepton_2_pT,lepton_2_eta,lepton_2_phi,missing_energy_magnitude,missing_energy_phi,MET_rel,axial_MET,M_R,M_TR_2,R,MT2,S_R,M_DELTA_R,dPhi_r_b,cos_thera_r1], hasIntercept=true), LogisticRegressionModel: uid=LogisticRegression_99162fba90c1, numClasses=2, numFeatures=18]\n","<class 'pyspark.ml.tuning.TrainValidationSplitModel'>\n","PipelineModel_7d200423be21\n","[RFormulaModel: uid=RFormula_59063e31b2ca, resolvedFormula=ResolvedRFormula(label=class, terms=[lepton_1_pT,lepton_1_eta,lepton_1_phi,lepton_2_pT,lepton_2_eta,lepton_2_phi,missing_energy_magnitude,missing_energy_phi,MET_rel,axial_MET,M_R,M_TR_2,R,MT2,S_R,M_DELTA_R,dPhi_r_b,cos_thera_r1], hasIntercept=true), LogisticRegressionModel: uid=LogisticRegression_99162fba90c1, numClasses=2, numFeatures=18]\n","<class 'pyspark.ml.tuning.TrainValidationSplitModel'>\n","PipelineModel_6a091ecbfb15\n","[RFormulaModel: uid=RFormula_59063e31b2ca, resolvedFormula=ResolvedRFormula(label=class, terms=[lepton_1_pT,lepton_1_eta,lepton_1_phi,lepton_2_pT,lepton_2_eta,lepton_2_phi,missing_energy_magnitude,missing_energy_phi,MET_rel,axial_MET,M_R,M_TR_2,R,MT2,S_R,M_DELTA_R,dPhi_r_b,cos_thera_r1], hasIntercept=true), LogisticRegressionModel: uid=LogisticRegression_99162fba90c1, numClasses=2, numFeatures=18]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yZvS_Vj3TWn0","executionInfo":{"status":"ok","timestamp":1603280186504,"user_tz":-660,"elapsed":106182,"user":{"displayName":"Duy Tung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXaQhxVwGl7g-mvwhXwJj3o9cSpPflw5NhM3rEYQ=s64","userId":"05049257213918528608"}},"outputId":"92d0e4b3-c721-496a-9cd4-eec238c4459a","colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Evaluate the trained model with the test set and report the performance\n","for tvsFitted in tvsFitteds:\n","  print('The area under ROC of the model with the test set is: ', evaluator.evaluate(tvsFitted.bestModel.transform(test_SUSY)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The area under ROC of the model with the test set is:  0.7496154274996407\n","The area under ROC of the model with the test set is:  0.7498346298756671\n","The area under ROC of the model with the test set is:  0.7496480146641429\n","The area under ROC of the model with the test set is:  0.7500984683473715\n","The area under ROC of the model with the test set is:  0.7498730746923208\n"],"name":"stdout"}]}]}